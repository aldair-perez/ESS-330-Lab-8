---
title: "hyperparameter-tuning"
subtitle: "ESS-330-Lab-8"
author:
- name: "Aldair Perez Marmolejo"
format: html
execute: 
  echo: true
---

# Data Set Up
```{r}
library(tidyverse)
library(tidymodels)
library(powerjoin)
library(recipes)
install.packages("sparklyr")
library(sparklyr)
library(workflows)
library(dials)
library(dplyr)
library(ggplot2)
library(patchwork)
```

```{r}
print(camels)
```

```{r}
camels <- na.omit(camels)
```

# Data Splitting
```{r}
set.seed(123)
start_split <- initial_split(camels, prop = 0.8)
trainer <- training(start_split)
tester <- testing(start_split)
```

# Feature Engineering
```{r}
camels_receta <- recipe(q_mean ~ ., data = trainer) %>%
  step_rm(gauge_lat, gauge_lon, id = "remove_location") %>%
  step_string2factor(all_nominal_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors())
```

# Resampling and Model Testing
```{r}
kamelz_cv <- vfold_cv(trainer, v = 10)
```

```{r}
arbol <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("regression")
bosque <- rand_forest() %>%
  set_engine("ranger") %>%
  set_mode("regression")
NOS <- boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("regression")
SVT <- svm_linear() %>%
  set_engine("kernlab") %>%
  set_mode("regression")
brains <- bag_mlp() %>%
  set_engine("nnet") %>%
  set_mode("regression")
```

## The 3 models I chose are arbol, bosque, and NOS.

```{r}
model_list <- list(dt = arbol, rf = bosque, xgb = NOS)
WTF_set <- workflow_set(preproc = list(classifier = camels_receta), models = model_list)

wf_results <- workflow_map(
  WTF_set,
  resamples = kamelz_cv,
  grid = 10,
  control = control_grid(save_pred = TRUE)
)
autoplot(wf_results)
```

```{r}
rank_results(wf_results, rank_metric = "rsq", select_best = TRUE)
```

## The model that performs best is boost (NOS). For the metric, rsq, xgb is the highest with 0.997. NOS uses boosted_tree, the engine is xgboost, and the mode is regression.

# Model Tuning
```{r}
NOS_tuners <- boost_tree(
  trees = tune(), learn_rate = tune(), tree_depth = tune()) %>%
  set_engine("xgboost") %>%
  set_mode("regression")
```

```{r}
NOS_workflow <- workflow() %>%
  add_model(NOS_tuners) %>%
  add_recipe(camels_receta)
```

```{r}
dials <- extract_parameter_set_dials(NOS_workflow)
dials$object
```

```{r}
set.seed(123)
my.grid <- dials |>
  grid_space_filling(paramters = dials, size = 20)

tuned_NOS <- tune_grid(
  NOS_workflow,
  resamples = kamelz_cv,
  grid = my.grid,
  control = control_grid(save_pred = TRUE, verbose = TRUE)
)
```
```{r}
model_params <-  tune_grid(
    NOS_workflow,
    resamples = kamelz_cv,
    grid = my.grid,
    metrics = metric_set(rmse, rsq, mae),
    control = control_grid(save_pred = TRUE)
  )
autoplot(model_params)
```

```{r}
collect_metrics(model_params) %>%
  arrange(mean)
```
```{r}
show_best(model_params, metric = "mae", n = 5)
```
## The top 5 from the show_best is shown. From the mean, it looks like 0.02079 is the best (because it's the lowest). The other top 4 are close to 0.0208 for the mean. They all had the lowest MAE from the model_params dataset.

```{r}
hp_best <- select_best(model_params, metric = "mae")
print(hp_best)
```

```{r}
fin_wrkflw <- finalize_workflow(NOS_workflow, hp_best)
print(fin_wrkflw)
```

# Final Model Verification
```{r}
final_fit <- last_fit(fin_wrkflw, split = start_split)
collect_metrics(final_fit)
```
## The RMSE is 0.0365, while the RSQ is 0.999. Since the RMSE value is low, and the RSQ value is close to 1, this means that this final_fit model is more on the accurate side, according to these 2 values produced.

```{r}
test_predictions <- collect_predictions(final_fit)
print(test_predictions)
```

```{r}
ggplot(test_predictions, aes(x = q_mean, y = .pred)) +
  geom_point(aes(color = .pred), alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE, color = "red", linetype = "dashed") +
  geom_abline(intercept = 0, slope = 1, color = "black", linetype = "solid") +
  scale_color_viridis_c(option = "D") +
  labs(
    title = "Predicted vs Actual Values",
    x = "Actual (Truth)",
    y = "Predicted",
    color = "Prediction"
  ) +
  theme_minimal()
```

# Building A Map
```{r}
final_model_fit <- fit(fin_wrkflw, data = camels)
full_pred <- augment(final_model_fit, new_data = camels)
full_pred <- full_pred %>%
  mutate(residual = (.pred - q_mean)^2)
map_predictions <- ggplot(full_pred, aes(x = gauge_lon, y = gauge_lat)) +
  geom_point(aes(color = .pred), size = 3, alpha = 0.8) +
  scale_color_viridis_c(option = "C") +
  labs(
    title = "Map of Predictions",
    color = "Predicted"
  ) +
  theme_minimal()
map_predictions
```

```{r}
map_residuals <- ggplot(full_pred, aes(x = gauge_lon, y = gauge_lat)) +
  geom_point(aes(color = residual), size = 3, alpha = 0.8) +
  scale_color_viridis_c(option = "A") +
  labs(
    title = "Map of Squared Residuals",
    color = "Residual"
  ) +
  theme_minimal()
map_residuals
```

```{r}
map_predictions / map_residuals
```

