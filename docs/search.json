[
  {
    "objectID": "hyperparameter-tuning.html",
    "href": "hyperparameter-tuning.html",
    "title": "hyperparameter-tuning",
    "section": "",
    "text": "options(repos = c(CRAN = \"https://cloud.r-project.org\"))"
  },
  {
    "objectID": "hyperparameter-tuning.html#the-3-models-i-chose-are-arbol-bosque-and-nos.",
    "href": "hyperparameter-tuning.html#the-3-models-i-chose-are-arbol-bosque-and-nos.",
    "title": "hyperparameter-tuning",
    "section": "The 3 models I chose are arbol, bosque, and NOS.",
    "text": "The 3 models I chose are arbol, bosque, and NOS.\n\nmodel_list &lt;- list(dt = arbol, rf = bosque, xgb = NOS)\nWTF_set &lt;- workflow_set(preproc = list(classifier = camels_receta), models = model_list)\n\nwf_results &lt;- workflow_map(\n  WTF_set,\n  resamples = kamelz_cv,\n  grid = 3,\n  control = control_grid(save_pred = TRUE)\n)\nautoplot(wf_results)\n\n\n\n\n\n\n\n\n\nrank_results(wf_results, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 6 × 9\n  wflow_id       .config   .metric   mean std_err     n preprocessor model  rank\n  &lt;chr&gt;          &lt;chr&gt;     &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 classifier_xgb Preproce… rmse    0.0678 0.00448     3 recipe       boos…     1\n2 classifier_xgb Preproce… rsq     0.926  0.00510     3 recipe       boos…     1\n3 classifier_rf  Preproce… rmse    0.0915 0.00918     3 recipe       rand…     2\n4 classifier_rf  Preproce… rsq     0.883  0.0173      3 recipe       rand…     2\n5 classifier_dt  Preproce… rmse    0.0946 0.00142     3 recipe       deci…     3\n6 classifier_dt  Preproce… rsq     0.859  0.00757     3 recipe       deci…     3"
  },
  {
    "objectID": "hyperparameter-tuning.html#the-model-that-performs-best-is-boost-nos.-for-the-metric-rsq-xgb-is-the-highest-with-0.997.-nos-uses-boosted_tree-the-engine-is-xgboost-and-the-mode-is-regression.",
    "href": "hyperparameter-tuning.html#the-model-that-performs-best-is-boost-nos.-for-the-metric-rsq-xgb-is-the-highest-with-0.997.-nos-uses-boosted_tree-the-engine-is-xgboost-and-the-mode-is-regression.",
    "title": "hyperparameter-tuning",
    "section": "The model that performs best is boost (NOS). For the metric, rsq, xgb is the highest with 0.997. NOS uses boosted_tree, the engine is xgboost, and the mode is regression.",
    "text": "The model that performs best is boost (NOS). For the metric, rsq, xgb is the highest with 0.997. NOS uses boosted_tree, the engine is xgboost, and the mode is regression."
  },
  {
    "objectID": "hyperparameter-tuning.html#the-top-5-from-the-show_best-is-shown.-from-the-mean-it-looks-like-0.02079-is-the-best-because-its-the-lowest.-the-other-top-4-are-close-to-0.0208-for-the-mean.-they-all-had-the-lowest-mae-from-the-model_params-dataset.",
    "href": "hyperparameter-tuning.html#the-top-5-from-the-show_best-is-shown.-from-the-mean-it-looks-like-0.02079-is-the-best-because-its-the-lowest.-the-other-top-4-are-close-to-0.0208-for-the-mean.-they-all-had-the-lowest-mae-from-the-model_params-dataset.",
    "title": "hyperparameter-tuning",
    "section": "The top 5 from the show_best is shown. From the mean, it looks like 0.02079 is the best (because it’s the lowest). The other top 4 are close to 0.0208 for the mean. They all had the lowest MAE from the model_params dataset.",
    "text": "The top 5 from the show_best is shown. From the mean, it looks like 0.02079 is the best (because it’s the lowest). The other top 4 are close to 0.0208 for the mean. They all had the lowest MAE from the model_params dataset.\n\nhp_best &lt;- select_best(model_params, metric = \"mae\")\nprint(hp_best)\n\n# A tibble: 1 × 4\n  trees tree_depth learn_rate .config              \n  &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;                \n1  2000          4     0.0379 Preprocessor1_Model08\n\n\n\nfin_wrkflw &lt;- finalize_workflow(NOS_workflow, hp_best)\nprint(fin_wrkflw)\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_rm()\n• step_string2factor()\n• step_unknown()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  trees = 2000\n  tree_depth = 4\n  learn_rate = 0.0379269019073225\n\nComputational engine: xgboost"
  },
  {
    "objectID": "hyperparameter-tuning.html#the-rmse-is-0.0365-while-the-rsq-is-0.999.-since-the-rmse-value-is-low-and-the-rsq-value-is-close-to-1-this-means-that-this-final_fit-model-is-more-on-the-accurate-side-according-to-these-2-values-produced.",
    "href": "hyperparameter-tuning.html#the-rmse-is-0.0365-while-the-rsq-is-0.999.-since-the-rmse-value-is-low-and-the-rsq-value-is-close-to-1-this-means-that-this-final_fit-model-is-more-on-the-accurate-side-according-to-these-2-values-produced.",
    "title": "hyperparameter-tuning",
    "section": "The RMSE is 0.0365, while the RSQ is 0.999. Since the RMSE value is low, and the RSQ value is close to 1, this means that this final_fit model is more on the accurate side, according to these 2 values produced.",
    "text": "The RMSE is 0.0365, while the RSQ is 0.999. Since the RMSE value is low, and the RSQ value is close to 1, this means that this final_fit model is more on the accurate side, according to these 2 values produced.\n\ntest_predictions &lt;- collect_predictions(final_fit)\nprint(test_predictions)\n\n# A tibble: 102 × 5\n   .pred id                .row q_mean .config             \n   &lt;dbl&gt; &lt;chr&gt;            &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;               \n 1 2.04  train/test split     1  2.17  Preprocessor1_Model1\n 2 1.89  train/test split    15  1.86  Preprocessor1_Model1\n 3 2.20  train/test split    17  2.24  Preprocessor1_Model1\n 4 2.18  train/test split    19  2.15  Preprocessor1_Model1\n 5 1.97  train/test split    28  2.03  Preprocessor1_Model1\n 6 1.13  train/test split    37  1.10  Preprocessor1_Model1\n 7 1.14  train/test split    38  1.16  Preprocessor1_Model1\n 8 0.946 train/test split    44  0.990 Preprocessor1_Model1\n 9 1.08  train/test split    46  1.14  Preprocessor1_Model1\n10 1.03  train/test split    47  1.00  Preprocessor1_Model1\n# ℹ 92 more rows\n\n\n\nggplot(test_predictions, aes(x = q_mean, y = .pred)) +\n  geom_point(aes(color = .pred), alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", linetype = \"dashed\") +\n  geom_abline(intercept = 0, slope = 1, color = \"black\", linetype = \"solid\") +\n  scale_color_viridis_c(option = \"D\") +\n  labs(\n    title = \"Predicted vs Actual Values\",\n    x = \"Actual (Truth)\",\n    y = \"Predicted\",\n    color = \"Prediction\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "hyperparameter-tuning.html#the-model-that-performs-best-is-boost-nos.-for-the-metric-rsq-xgb-is-the-highest-with-0.92.-nos-uses-boosted_tree-the-engine-is-xgboost-and-the-mode-is-regression.",
    "href": "hyperparameter-tuning.html#the-model-that-performs-best-is-boost-nos.-for-the-metric-rsq-xgb-is-the-highest-with-0.92.-nos-uses-boosted_tree-the-engine-is-xgboost-and-the-mode-is-regression.",
    "title": "hyperparameter-tuning",
    "section": "The model that performs best is boost (NOS). For the metric, rsq, xgb is the highest with 0.92. NOS uses boosted_tree, the engine is xgboost, and the mode is regression.",
    "text": "The model that performs best is boost (NOS). For the metric, rsq, xgb is the highest with 0.92. NOS uses boosted_tree, the engine is xgboost, and the mode is regression."
  },
  {
    "objectID": "hyperparameter-tuning.html#the-top-3-from-the-show_best-is-shown.-from-the-mean-it-looks-like-0.04064-is-the-best-because-its-the-lowest.-the-others-are-noticeablly-higher.-they-all-had-the-lowest-mae-from-the-model_params-dataset.",
    "href": "hyperparameter-tuning.html#the-top-3-from-the-show_best-is-shown.-from-the-mean-it-looks-like-0.04064-is-the-best-because-its-the-lowest.-the-others-are-noticeablly-higher.-they-all-had-the-lowest-mae-from-the-model_params-dataset.",
    "title": "hyperparameter-tuning",
    "section": "The top 3 from the show_best is shown. From the mean, it looks like 0.04064 is the best (because it’s the lowest). The others are noticeablly higher. They all had the lowest MAE from the model_params dataset.",
    "text": "The top 3 from the show_best is shown. From the mean, it looks like 0.04064 is the best (because it’s the lowest). The others are noticeablly higher. They all had the lowest MAE from the model_params dataset.\n\nhp_best &lt;- select_best(model_params, metric = \"mae\")\nprint(hp_best)\n\n# A tibble: 1 × 4\n  trees tree_depth learn_rate .config             \n  &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;               \n1  2000         15      0.316 Preprocessor1_Model3\n\n\n\nfin_wrkflw &lt;- finalize_workflow(NOS_workflow, hp_best)\nprint(fin_wrkflw)\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_rm()\n• step_string2factor()\n• step_unknown()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  trees = 2000\n  tree_depth = 15\n  learn_rate = 0.316227766016838\n\nComputational engine: xgboost"
  },
  {
    "objectID": "hyperparameter-tuning.html#the-rmse-is-0.058-while-the-rsq-is-0.936.-since-the-rmse-value-is-low-and-the-rsq-value-is-close-to-1-this-means-that-this-final_fit-model-is-accurate-to-some-extent-according-to-these-2-values-produced.",
    "href": "hyperparameter-tuning.html#the-rmse-is-0.058-while-the-rsq-is-0.936.-since-the-rmse-value-is-low-and-the-rsq-value-is-close-to-1-this-means-that-this-final_fit-model-is-accurate-to-some-extent-according-to-these-2-values-produced.",
    "title": "hyperparameter-tuning",
    "section": "The RMSE is 0.058, while the RSQ is 0.936. Since the RMSE value is low, and the RSQ value is close to 1, this means that this final_fit model is accurate to some extent, according to these 2 values produced.",
    "text": "The RMSE is 0.058, while the RSQ is 0.936. Since the RMSE value is low, and the RSQ value is close to 1, this means that this final_fit model is accurate to some extent, according to these 2 values produced.\n\ntest_predictions &lt;- collect_predictions(final_fit)\nprint(test_predictions)\n\n# A tibble: 102 × 5\n   .pred id                .row runoff_ratio .config             \n   &lt;dbl&gt; &lt;chr&gt;            &lt;int&gt;        &lt;dbl&gt; &lt;chr&gt;               \n 1 0.545 train/test split     1        0.602 Preprocessor1_Model1\n 2 0.514 train/test split    15        0.543 Preprocessor1_Model1\n 3 0.584 train/test split    17        0.570 Preprocessor1_Model1\n 4 0.601 train/test split    19        0.564 Preprocessor1_Model1\n 5 0.506 train/test split    28        0.550 Preprocessor1_Model1\n 6 0.347 train/test split    37        0.337 Preprocessor1_Model1\n 7 0.369 train/test split    38        0.352 Preprocessor1_Model1\n 8 0.305 train/test split    44        0.332 Preprocessor1_Model1\n 9 0.343 train/test split    46        0.353 Preprocessor1_Model1\n10 0.336 train/test split    47        0.329 Preprocessor1_Model1\n# ℹ 92 more rows\n\n\n\nggplot(test_predictions, aes(x = runoff_ratio, y = .pred)) +\n  geom_point(aes(color = .pred), alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", linetype = \"dashed\") +\n  geom_abline(intercept = 0, slope = 1, color = \"black\", linetype = \"solid\") +\n  scale_color_viridis_c(option = \"D\") +\n  labs(\n    title = \"Predicted vs Actual Values\",\n    x = \"Actual (Truth)\",\n    y = \"Predicted\",\n    color = \"Prediction\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'"
  }
]